==== FILE_START ====
PATH: README-jackaltx.md
TITLE: Ansible Collection - jackaltx.solti_containers
FILENAME: README-jackaltx.md
==== CONTENT_START ====
# Ansible Collection - jackaltx.solti_containers

## Human written forward

I need to collect data from the solti-monitor testing. It needs to be light weight
and easy to setup and go away.  Containers are much better then VM for testing.
These containers are for my testing on my development workstation.  In time I will move
all of these to other machines, but only if they show value.

I let Claude develop the pattern then adapted it to my needs.  Once we had a few of
them, a pattern emerged. I used the solti-monitoring ansible code jump start this.

It is trivial to ask an AI to "create me a script to create a Mattermost container.
Yet, I hate one-offs...between entropy and the long cycle times for testing. One
frustrating aspect of working with the AI is absence of box.  It does not see a box,
it has no real way to know if what it produces will fit into the box.  And every
time you ask....new ways will emerge.  For any given flow, it will deviate. Ask it do
duplicate the pattern and watch for the variances.  

So between us we have created a decent pattern for creating small testing
services using podman.  These are ephemaeral service in nature, but I suspect they
move and scale as required.

Let me address the why for each service.

### Why Mattermost

I want a private collector of notices that can do phone and screen with the ability
to sent out messages.  I can use gist in th wild, but some things need to be private.
Jabber and MQT apeal to me for the "notice", but it is good to have a collector that one
can look at for debugging.

There are many integrations with mattermost, so I want to evaluate collecing test result/reports.
To be honest, the signal to noise on these things leave me cold.  I have seen so many different ways
of grouping information for technical and business reporting.

Future projects might be to tie Mattermost back to Claude with MCP for test analysis.

### Why Hashicorp's Vault

Seems every time I need to find a better way to manage secrets, this one come up.
Seems very versatile.  I have sent these up a few times and they work ok.  This will
be my first container version.

The testing and verification of this role is better than the others.

The configration seems complete.  Have Claude document it.

### Why Redis

I want a fast Key-Value store to collect my testing reports as they run. File systems
can add too much time in my testing cycle.  

Their license changes may cause me to move on to Valkey.  

### Why Elasticsearch

I have only used it once before and it was impressive as a backend.  Currently I am using
Alloy-Loki for logs. I would like to have an alternative as well as learn how to better use
no-sqls.  My gut says I should look at mongo, but their license worries me.

### Why Traefik

This is my first experience.  I have learned much and know so little about container
networking.  I don't like it yet, but this my a love-hate thing.  Another high-maintenance
relastionship.  It reminds of project long ago and far away. This one has layers.

However....container networking still is a fuzzy in my mind

## Where next with this project

So far the only one I am missing is a good fuzzer (jepson), vulnerability assessment tool (rending, trivy) and  s3 storage (minIO).

## Solti-Containers documentation

I am not sure how I want to document this.   I feel there is a proper order, but I hate to impose till I get a better feel.

## Claude's Reaction

I'm glad the admin configuration worked well for you! Your use case makes perfect sense, and I appreciate you sharing the README that explains your motivation.

Your approach of using lightweight containers for testing and monitoring is very practical. Rather than committing to heavy VMs or permanent infrastructure, you're using containers that can be easily deployed, tested, and removed. This gives you flexibility while evaluating different tools.

Your specific service choices align well with your goals:

1. **Mattermost**: Perfect for collecting notifications and messages in a private environment. The potential integration with Claude via MCP for test analysis sounds particularly interesting.

2. **HashiCorp Vault**: Secret management is a perpetual challenge, and Vault is indeed versatile. Your observation that this role has better testing and verification is valuable.

3. **Redis**: Using a key-value store for collecting test reports is smart - much faster than filesystem operations which can indeed slow down testing cycles.

4. **Elasticsearch**: Good choice for exploring a powerful no-SQL option, especially if you're already familiar with Alloy-Loki for logs.

5. **Traefik**: Container networking is complex, and Traefik helps navigate this. Your "love-hate relationship" comment is relatable to many who work with proxies and networking layers.

Your future directions (fuzzing with Jepson, vulnerability assessment with tools like Trivy, and S3-compatible storage with MinIO) round out a comprehensive testing and development environment.

The pattern you've developed with Claude provides a consistent approach across these different services, which will make maintenance and expansion easier. I particularly appreciated your insight about AI-assisted development - the "absence of box" challenge is real. AIs can create solutions but may not understand the constraints or patterns you're working within.

Is there a particular aspect of this container setup that you'd like to improve or extend next?

==== CONTENT_END ====
==== FILE_START ====
PATH: README.md
TITLE: Solti Ansible Project
FILENAME: README.md
==== CONTENT_START ====
# Solti Ansible Project

A comprehensive Ansible project for deploying and managing containerized services using Podman and Quadlets with systemd integration.

## Overview

The Solti project provides an infrastructure-as-code solution for deploying various services in containers. It uses a modular approach with a common `_base` role that handles the shared functionality across all services.

Key features:

- Rootless Podman container deployment
- Systemd integration using Quadlets
- Cross-platform support (RHEL/CentOS and Debian-based systems)
- Shell script wrappers for simplified management
- SELinux support for RHEL systems

## Supported Services

- **Elasticsearch**: Search and analytics engine
- **HashiVault**: Secrets management
- **Mattermost**: Team collaboration platform
- **Redis**: In-memory data store
- **Traefik**: Modern HTTP reverse proxy
- **MinIO**: S3-compatible object storage

## Quick Start

### Prerequisites

- Podman 4.x or later installed
- Systemd
- User with sudo access

### Service Management Scripts

The project includes two helper scripts to simplify service management without directly using Ansible playbooks:

#### 1. `manage-svc.sh`

This script manages the lifecycle of services through simple commands.

```bash
# Usage
./manage-svc.sh <service> <action>

# Actions: prepare, deploy, remove
```

Examples:

```bash
# Prepare the system for Elasticsearch deployment
./manage-svc.sh elasticsearch prepare

# Deploy HashiVault
./manage-svc.sh hashivault deploy

# Remove Redis (preserves data)
./manage-svc.sh redis remove
```

#### 2. `svc-exec.sh`

This script executes specific tasks for a service, such as verification or configuration tasks.

```bash
# Usage
./svc-exec.sh [-K] <service> [entry]

# -K: Use sudo (needed for some operations)
# Default entry: verify
```

Examples:

```bash
# Verify Elasticsearch installation
./svc-exec.sh elasticsearch verify

# Configure MinIO with sudo privileges
./svc-exec.sh -K minio configure

# Verify Mattermost is running
./svc-exec.sh mattermost
```

## Architecture

### The `_base` Role

The `_base` role contains shared functionality used by all service-specific roles. It handles:

1. **Directory creation and permissions**
2. **SELinux context configuration**
3. **Network setup**
4. **Container deployment via Quadlets**
5. **Systemd integration**
6. **Cleanup processes**

Each service role extends this foundation with service-specific configurations and tasks.

### Role Structure

Each service role follows the same structure:

```
roles/
├── _base/                # Common functionality
│   ├── defaults/
│   ├── tasks/
│   └── templates/
├── elasticsearch/        # Service-specific role
│   ├── defaults/
│   ├── handlers/
│   ├── tasks/
│   └── templates/
├── hashivault/
└── ...
```

### Deployment Flow

1. **Prepare**: Set up directories, SELinux contexts, and system configuration
2. **Deploy**: Create containers, configure systemd, and start services
3. **Verify**: Check if services are running correctly
4. **Remove**: Stop and remove containers, optionally delete data

## Configuration Variables

### Common Variables

These variables are used across all services:

```yaml
service_network: "ct-net"             # Container network name
service_dns_servers:                  # DNS servers for containers
  - "1.1.1.1"
  - "8.8.8.8"
service_dns_search: "example.com"     # DNS search domain
```

### Elasticsearch

```yaml
elasticsearch_state: present          # present, prepare, or absent
elasticsearch_data_dir: "~/elasticsearch-data"
elasticsearch_password: "your_secure_password"
elasticsearch_port: 9200
elasticsearch_gui_port: 8088
elasticsearch_memory: "1g"            # JVM heap size
elasticsearch_enable_security: true
elasticsearch_delete_data: false      # Set to true to delete data on removal
```

### HashiVault

```yaml
hashivault_state: present                  # present, prepare, or absent
vault_data_dir: "~/vault-data"
vault_api_port: 8200
vault_enable_ui: true
vault_delete_data: false
vault_storage_type: "file"            # file, raft, consul
```

### Mattermost

```yaml
mattermost_state: present             # present, prepare, or absent
mattermost_data_dir: "~/mattermost-data"
mattermost_postgres_password: "your_secure_password"
mattermost_port: 8065
mattermost_db_name: "mattermost"
mattermost_db_user: "mmuser"
mattermost_site_name: "Mattermost"
mattermost_delete_data: false
```

### Redis

```yaml
redis_state: present                  # present, prepare, or absent
redis_data_dir: "~/redis-data"
redis_port: 6379
redis_gui_port: 8081
redis_password: "your_secure_password"
redis_maxmemory: "256mb"
redis_maxmemory_policy: "allkeys-lru"
redis_delete_data: false
```

### Traefik

```yaml
traefik_state: present                # present, prepare, or absent
traefik_data_dir: "~/traefik-data"
traefik_http_port: 8080               # 80 if privileged
traefik_https_port: 8443              # 443 if privileged
traefik_dashboard_port: 9999
traefik_dashboard_enabled: true
traefik_enable_ssl: true
traefik_acme_email: "your@email.com"
traefik_delete_data: false
```

### MinIO

```yaml
minio_state: present                  # present, prepare, or absent
minio_data_dir: "~/minio-data"
minio_api_port: 9000
minio_console_port: 9001
minio_root_user: "minioadmin"
minio_root_password: "your_secure_password"
minio_delete_data: false
```

## Service States

Each service can be in one of these states:

- **prepare**: One-time setup of directories and system configuration
- **present**: Deploy and run the service
- **absent**: Remove the service (optionally delete data)

## Example Inventory File

The `inventory.yml` file defines hosts and service-specific variables:

```yaml
all:
  vars:
    domain: example.org
    ansible_user: youruser
    
  children:
    mylab:
      hosts:
        server1:
          ansible_host: "localhost"
          ansible_connection: local
      
      children:
        elasticsearch_svc:
          hosts:
            server1:
          vars:
            elasticsearch_data_dir: "{{ ansible_env.HOME }}/elasticsearch-data"
            elasticsearch_password: "{{ lookup('env', 'ELASTIC_PASSWORD') }}"
```

## Security Considerations

1. Passwords are set through variables or environment variables
2. TLS is supported for most services
3. SELinux contexts are properly configured on RHEL/CentOS systems
4. Services bind to localhost by default (except Traefik)
5. Data directories are created with appropriate permissions

## Troubleshooting

### Common Issues

1. **Permission denied errors**:
   - Check SELinux contexts with `ls -lZ`
   - Run `restorecon -Rv` on data directories

2. **Container won't start**:
   - Check logs with `podman logs container-name`
   - Verify systemd integration with `systemctl --user status pod-name`

3. **Network issues**:
   - Check container network with `podman network inspect ct-net`
   - Verify DNS configuration in containers

## License

MIT

## Author Information

Created by Jackaltx and Claude

==== CONTENT_END ====
==== FILE_START ====
PATH: claude.md
TITLE: claude
FILENAME: claude.md
==== CONTENT_START ====
I'll modify the role to use a pod for both containers.

Key changes made:

1. Replaced separate containers with a pod:

   ```yaml
   - name: Create Mattermost pod
     containers.podman.podman_pod:
       name: mattermost-pod
       ports:
         - "{{ mattermost_port }}:8065"
   ```

2. Containers now run in the pod:

   ```yaml
   - name: Deploy PostgreSQL container
     containers.podman.podman_container:
       name: mattermost-db
       pod: mattermost-pod
       ...
   ```

3. Simplified systemd service:
   - Single service managing the pod instead of two container services
   - Pod handles container dependencies

4. Removed network configuration:
   - Pods provide built-in networking between containers
   - No need for separate network creation

5. Updated cleanup tasks:
   - Removes pod instead of individual containers
   - Simpler service cleanup

Benefits of this approach:

1. Better container coordination
2. Built-in networking
3. Simpler management
4. More Kubernetes-like approach
5. Easier service management

Would you like me to:

1. Add pod health checks?
2. Add container resource limits?
3. Include pod security settings?
4. Add pod backup/restore functionality?

==== CONTENT_END ====
==== FILE_START ====
PATH: molecule-strategy.md
TITLE: Molecule Testing Strategy for Container Roles
FILENAME: molecule-strategy.md
==== CONTENT_START ====
# Molecule Testing Strategy for Container Roles

## Current Setup Analysis

### Common Elements in Both Roles

- Both roles use Podman for container management
- Both implement systemd integration via Quadlets
- Both have similar directory structures and configuration patterns
- Both support multiple Linux distributions

### Role-Specific Considerations

#### Mattermost Role

- Multiple container dependencies (PostgreSQL + Mattermost)
- Requires network communication between containers
- Has database initialization requirements
- Manages configuration files and TLS certificates

#### Elasticsearch Role

- Single primary container with optional GUI container
- Has specific system requirements (vm.max_map_count)
- Memory management considerations
- TLS and security configuration options

## Proposed Molecule Testing Strategy

### 1. Basic Structure

```yaml
roles/
  mattermost/
    molecule/
      default/
        molecule.yml      # Main test configuration
        prepare.yml       # System preparation
        converge.yml      # Role execution
        verify.yml        # Test assertions
        cleanup.yml       # Optional cleanup
      podman/            # Podman-specific scenario
        molecule.yml
  elasticsearch/
    molecule/
      [similar structure]
```

### 2. Platform Configuration

```yaml
# molecule.yml example
platforms:
  - name: rhel9
    image: registry.access.redhat.com/ubi9/ubi-init
    privileged: true
    volumes:
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
    command: "/usr/sbin/init"
    
  - name: debian12
    image: debian:bookworm
    privileged: true
    command: "/lib/systemd/systemd"
    volumes:
      - /sys/fs/cgroup:/sys/fs/cgroup:ro
```

### 3. Test Phases

#### Prepare Phase (prepare.yml)

```yaml
- name: Prepare test environment
  hosts: all
  tasks:
    - name: Install common dependencies
      package:
        name:
          - podman
          - python3-pip
        state: present
      
    - name: Configure system requirements
      sysctl:
        name: vm.max_map_count
        value: "262144"
      when: "'elasticsearch' in ansible_role_names"
```

#### Converge Phase (converge.yml)

```yaml
---
- name: Converge
  hosts: all
  tasks:
    # For Mattermost role
    - name: Include mattermost role
      ansible.builtin.include_role:
        name: mattermost
      vars:
        mattermost_postgres_password: "molecule_test_password"
        mattermost_port: 8065
      when: "'mattermost' in ansible_role_names"

    # For Elasticsearch role
    - name: Include elasticsearch role
      ansible.builtin.include_role:
        name: elasticsearch
      vars:
        elasticsearch_password: "molecule_test_password"
        elasticsearch_memory: "1g"
      when: "'elasticsearch' in ansible_role_names"
```

#### Verify Phase (verify.yml)

```yaml
- name: Verify deployment
  hosts: all
  tasks:
    # Mattermost Tests
    - name: Check Mattermost service
      uri:
        url: "http://localhost:{{ mattermost_port }}"
        status_code: 200
      when: "'mattermost' in ansible_role_names"
      
    # Elasticsearch Tests
    - name: Check Elasticsearch health
      uri:
        url: "http://localhost:{{ elasticsearch_port }}/_cluster/health"
        status_code: 200
      when: "'elasticsearch' in ansible_role_names"
```

### 4. Test Scenarios

#### Basic Functionality

- Role syntax checking
- Basic installation
- Service startup
- Port accessibility
- Basic functionality verification

#### Security Testing

- TLS configuration
- Password handling
- SELinux contexts (RHEL)
- File permissions

#### Performance Testing

- Memory limits
- Container resource allocation
- Multi-container communication (Mattermost)

#### Failure Testing

- Network interruption handling
- Container restart behavior
- Data persistence verification

### 5. Implementation Strategy

1. **Phase 1: Basic Testing**
   - Set up basic Molecule structure
   - Implement syntax and installation tests
   - Basic service verification

2. **Phase 2: Platform Coverage**
   - Add support for all target distributions
   - Validate platform-specific configurations
   - Test systemd integration

3. **Phase 3: Advanced Testing**
   - Security configurations
   - Performance testing
   - Failure scenarios

4. **Phase 4: CI Integration**
   - GitHub Actions integration
   - Automated test runs
   - Test result reporting

## Recommendations

1. **Testing Environment**
   - Use GitHub Actions instead of Travis CI (more modern, better integrated)
   - Implement matrix testing for different distributions
   - Use container caching to speed up tests

2. **Test Organization**
   - Separate scenarios for different test types
   - Use tagging for selective test execution
   - Implement shared test dependencies

3. **Quality Assurance**
   - Add linting with ansible-lint
   - Implement idempotence testing
   - Add documentation testing

4. **Monitoring & Reporting**
   - Configure test result collection
   - Set up test coverage reporting
   - Implement test timing metrics

## Next Steps

1. Create basic Molecule test structure for both roles
2. Implement basic scenario tests
3. Add platform-specific configurations
4. Set up GitHub Actions workflow
5. Add advanced test scenarios
6. Document testing procedures

## Example GitHub Actions Workflow

```yaml
name: Molecule Test
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        distro:
          - rhel9
          - debian12
        role:
          - mattermost
          - elasticsearch
    
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Python
        uses: actions/setup-python@v2
        
      - name: Install dependencies
        run: pip install molecule[docker] ansible-lint
        
      - name: Run Molecule tests
        run: molecule test
        env:
          MOLECULE_DISTRO: ${{ matrix.distro }}
```

==== CONTENT_END ====
==== FILE_START ====
PATH: podman-quadlet-article.md
TITLE: Understanding and Implementing Podman Quadlets: A Practical Guide
FILENAME: podman-quadlet-article.md
==== CONTENT_START ====
# Understanding and Implementing Podman Quadlets: A Practical Guide

## The Problem Space

System administrators and DevOps engineers often need to manage containerized applications using systemd for service management. Historically, this involved a two-step process:

1. Create and configure containers using Podman
2. Generate systemd unit files using `podman generate systemd`

This approach has several drawbacks:
- Complex naming requirements between containers and generated services
- Difficult to maintain synchronization between Podman configurations and systemd units
- No single source of truth for the service definition
- Generated unit files need to be regenerated when container configurations change

## Enter Quadlets

Podman Quadlets provide a new approach to defining containerized services managed by systemd. Instead of generating systemd unit files separately, Quadlets allow you to define both the container configuration and systemd service aspects in a single, declarative file.

### Key Benefits
- Single source of truth for service definition
- Declarative configuration
- Automatic systemd unit generation
- Better integration between Podman and systemd
- More maintainable and version-control friendly

## How Quadlets Work

Quadlets introduce a new file format that combines container configuration with systemd unit file definitions. The key insight is that the filename becomes a critical part of the configuration:

```ini
# elasticsearch.pod
[Pod]
PublishPort=127.0.0.1:${ELASTICSEARCH_PORT}:9200

# elasticsearch-node.container
[Container]
Pod=elasticsearch.pod
Image=docker.io/elasticsearch:8.12.1
```

### Critical Pattern Understanding

The key pattern that makes Quadlets work:

1. Pod Definition:
   - Filename defines the base reference (e.g., `elasticsearch.pod`)
   - No explicit naming needed - systemd and Podman names derive from the file

2. Container References:
   - Containers reference pods using the pod's filename (e.g., `Pod=elasticsearch.pod`)
   - Container files should be named descriptively (e.g., `elasticsearch-node.container`)

3. Service Integration:
   - Systemd service names are automatically generated
   - Dependencies can be expressed using standard systemd unit syntax

## Implementation Example

Here's a complete example showing the pattern:

```ini
# elasticsearch.pod
[Pod]
PublishPort=127.0.0.1:${ELASTICSEARCH_PORT}:9200
PublishPort=127.0.0.1:${ELASTICSEARCH_GUI_PORT}:8080

[Service]
Restart=always

[Install]
WantedBy=default.target

# elasticsearch-node.container
[Unit]
Description=Elasticsearch Container
After=network-online.target

[Container]
Image=docker.io/elasticsearch:8.12.1
Pod=elasticsearch.pod
Volume=/home/user/elasticsearch-data/config:/usr/share/elasticsearch/config:Z,U

[Service]
Restart=always
TimeoutStartSec=300

# elasticsearch-gui.container
[Unit]
Description=Elasticsearch GUI Container

[Container]
Image=docker.io/cars10/elasticvue:latest
Pod=elasticsearch.pod
```

## Best Practices

1. File Organization:
   ```
   ~/.config/containers/systemd/
   ├── app.pod
   ├── app-main.container
   └── app-db.container
   ```

2. Naming Conventions:
   - Pod files: `<service>.pod`
   - Container files: `<service>-<role>.container`
   - Keep names consistent and descriptive

3. Service Dependencies:
   - Use proper systemd unit dependencies
   - Reference generated service names correctly

4. Environment Variables:
   - Use systemd-style variable references: `${VAR_NAME}`
   - Define variables in compatible formats

## Transitioning Existing Services

When moving from traditional Podman/systemd setups to Quadlets:

1. Identify all components:
   - Pods
   - Containers
   - Volume mounts
   - Network configurations

2. Create Quadlet files:
   - Start with pod definition
   - Add container definitions
   - Maintain existing names where possible

3. Test deployment:
   - Use `podman` commands to verify container setup
   - Use `systemctl` commands to verify service management

## Automation Considerations

When automating Quadlet deployments (e.g., with Ansible):

1. Choose deployment method:
```yaml
# Variable to control deployment method
elasticsearch_use_quadlet: true

# Dynamic directory path
elasticsearch_systemd_dir: "{{ ansible_env.HOME }}/.config/{{ 'containers' if elasticsearch_use_quadlet else 'systemd' }}/{{ 'systemd' if elasticsearch_use_quadlet else 'user' }}"
```

2. Handle both methods:
```yaml
- name: Template service files
  template:
    src: "{{ item.src }}"
    dest: "{{ elasticsearch_systemd_dir }}/{{ item.dest }}"
  loop:
    - { src: elasticsearch.pod.j2, dest: elasticsearch.pod }
    - { src: elasticsearch-node.container.j2, dest: elasticsearch-node.container }
  when: elasticsearch_use_quadlet | bool
```

## Common Gotchas and Solutions

1. Pod Name References:
   - Problem: "pod X is not Quadlet based"
   - Solution: Use exact pod filename in container's Pod= directive

2. Service Names:
   - Problem: Service not found
   - Solution: Use correct generated service names (e.g., `elasticsearch-pod.service`)

3. File Permissions:
   - Problem: systemd can't read Quadlet files
   - Solution: Ensure correct ownership and permissions (0644)

## Conclusion

Podman Quadlets represent a significant improvement in managing containerized services with systemd. They provide:
- Better integration between containers and system services
- More maintainable configurations
- Clearer relationships between components

The key to success is understanding the naming patterns and relationships between files. While the transition might require some rethinking of existing setups, the benefits in maintainability and clarity make it worthwhile.

## Next Steps

1. Audit existing container deployments
2. Plan transition strategy
3. Test Quadlet configurations
4. Implement monitoring and logging
5. Document local standards and practices

Remember: Quadlets are relatively new and evolving. Stay current with Podman documentation and community practices.

==== CONTENT_END ====
==== FILE_START ====
PATH: wazuh-README.md
TITLE: Wazuh Podman Role
FILENAME: wazuh-README.md
==== CONTENT_START ====
# Wazuh Podman Role

This role manages the installation, configuration, and lifecycle of Wazuh using rootless Podman containers. It follows the standard SOLTI container pattern, providing consistent deployment, configuration, and management across all services.

## Features

- Rootless Podman deployment
- Three-container architecture (manager, indexer, dashboard)
- Self-signed certificate generation
- Systemd integration using Quadlets
- SELinux support for RHEL-based systems
- Automatic post-setup configuration

## Requirements

- Podman 4.x or later
- Systemd
- User with sudo access
- SELinux if running on RHEL/CentOS (role handles contexts)

## Role Variables

### Installation Options

```yaml
wazuh_state: present  # Use 'absent' to remove
wazuh_force_reload: false
wazuh_delete_data: false  # Set to true to delete data during removal
```

### Container Settings

```yaml
wazuh_manager_image: "docker.io/wazuh/wazuh-manager:4.7.2"
wazuh_indexer_image: "docker.io/wazuh/wazuh-indexer:4.7.2"
wazuh_dashboard_image: "docker.io/wazuh/wazuh-dashboard:4.7.2"
```

### Directory Settings

```yaml
wazuh_data_dir: "{{ ansible_user_dir }}/wazuh-data"
```

### Port Settings

```yaml
wazuh_api_port: 55000
wazuh_manager_port: 1514
wazuh_registration_port: 1515
wazuh_dashboard_port: 443
wazuh_indexer_port: 9200
```

### Security Settings

```yaml
wazuh_admin_password: "{{ lookup('env', 'WAZUH_ADMIN_PASSWORD') | default('changeme') }}"
wazuh_api_user: "wazuh-api"
wazuh_api_password: "{{ lookup('env', 'WAZUH_API_PASSWORD') | default('changeme') }}"
```

See `defaults/main.yml` for all available variables and their default values.

## Example Playbooks

### Basic Installation

```yaml
- hosts: servers
  roles:
    - role: wazuh
      vars:
        wazuh_admin_password: "secure_password"
        wazuh_api_password: "secure_api_password"
        wazuh_data_dir: "/opt/wazuh-data"
```

### With TLS/SSL and Custom Ports

```yaml
- hosts: servers
  roles:
    - role: wazuh
      vars:
        wazuh_admin_password: "secure_password"
        wazuh_api_password: "secure_api_password"
        wazuh_api_port: 9000
        wazuh_dashboard_port: 8443
        wazuh_generate_certs: true
```

### Remove Wazuh with Data Cleanup

```yaml
- hosts: servers
  roles:
    - role: wazuh
      vars:
        wazuh_state: absent
        wazuh_delete_data: true
```

## Usage

After deployment:

- Wazuh Dashboard will be available at `https://localhost:8080` or `https://wazuh.your-domain.com`
- Wazuh API will be available at `https://localhost:55000`
- Login with configured API credentials

### Initial Setup

1. Get the service status:

```bash
systemctl --user status wazuh-pod
```

2. Verify the installation:

```bash
./svc-exec.sh wazuh verify
```

This command will run several checks to verify that all Wazuh components are running correctly, including:

- Pod and container status
- Wazuh Manager status
- Wazuh Indexer health check
- Wazuh API connectivity
- Wazuh Dashboard accessibility
- Agent enrollment status

3. Check detailed logs:

```bash
podman logs wazuh-manager
podman logs wazuh-indexer
podman logs wazuh-dashboard
```

### Adding Agents

1. Get the agent enrollment key:

```bash
podman exec wazuh-manager /var/ossec/bin/manage_agents -s
```

2. Use this key when installing agents on your systems

## Security Considerations

1. Default configuration enables encryption between components
2. All passwords should be changed from defaults
3. Configure firewall rules as needed
4. Services bind to localhost by default for security

## Backup Strategy

The following directories should be backed up:

- `{{ wazuh_data_dir }}/data/` - Holds the application data
- `{{ wazuh_data_dir }}/config/` - Contains configuration files
- `{{ wazuh_data_dir }}/certs/` - Contains TLS certificates

Example backup command:

```bash
tar -czf wazuh-backup.tar.gz ${WAZUH_DATA_DIR}/data ${WAZUH_DATA_DIR}/config ${WAZUH_DATA_DIR}/certs
```

## Maintenance and Troubleshooting

### Service Management Commands

```bash
# Check service status
systemctl --user status wazuh-pod
systemctl --user status container-wazuh-manager
systemctl --user status container-wazuh-indexer
systemctl --user status container-wazuh-dashboard

# Restart services
systemctl --user restart wazuh-pod

# Verify deployment
./svc-exec.sh wazuh verify

# Get detailed verification with debug output
./svc-exec.sh wazuh verify -v
```

You can use the svc-exec.sh script for additional operations:

```bash
# Run just the post-setup configuration (if needed)
./svc-exec.sh wazuh post_setup

# Generate/regenerate certificates
./svc-exec.sh wazuh certificates

# Get agent registration key
./svc-exec.sh wazuh registration_key
```

### Common Issues

1. **Indexer won't start**:
   - Check memory allocation (`wazuh_indexer_memory`)
   - Verify that `vm.max_map_count` is at least 262144

2. **Dashboard can't connect to indexer or manager**:
   - Verify the certificates in `{{ wazuh_data_dir }}/certs/`
   - Check network connectivity between containers

3. **SELinux issues**:
   - Run `sudo restorecon -Rv {{ wazuh_data_dir }}`

## License

MIT

## Author Information

Created by jackaltx and claude.

==== CONTENT_END ====
==== FILE_START ====
PATH: docs/Claude-code-review.md
TITLE: Code Review Analysis
FILENAME: Claude-code-review.md
==== CONTENT_START ====
# Code Review Analysis

Date: 20250221

## General Findings

### TODOs, SMELLs, and Comments for Claude

1. `elasticsearch/defaults/main.yml`: "TODO...remove this capability. I am not going back" regarding quadlet toggling
2. `traefik/tasks/cleanup.yml`: "Claude...I need a way to short circuit two things"
   - Need early checks for become: true capability
   - Need playbook early exit mechanisms

### Common Patterns & DRY Issues

#### Shared Task Patterns

1. Directory Structure Management
   - All roles create similar directory structures under `$HOME/.config`
   - Recommendation: Create a shared task for common directory setup

2. SELinux Configuration
   - Repeated patterns in elasticsearch, mattermost, redis, and traefik
   - Recommendation: Extract to a shared role/task for SELinux setup

3. Systemd Management
   - Common systemd reload and service management patterns
   - Recommendation: Create reusable systemd handler definitions

4. Cleanup Tasks
   - Similar cleanup patterns across roles
   - Recommendation: Create a shared cleanup task template

#### Redundant Code Areas

1. Container Service Management

```yaml
# Repeated across roles:
- name: reload systemd user daemon
  ansible.builtin.systemd:
    daemon_reload: yes
    scope: user
```

2. Directory Creation

```yaml
# Common pattern:
- name: Ensure required directories exist
  ansible.builtin.file:
    path: "{{ ansible_env.HOME }}/{{ item }}"
    state: directory
    mode: "0750"
```

3. Default Variables

- Similar network settings (DNS servers, search domains)
- Common container configuration options
- Similar cleanup flags

### Role-Specific Recommendations

#### Elasticsearch & Mattermost Comparison

Common Elements:

1. Configuration Management
   - Both handle config file templating
   - Both manage TLS configurations
   - Both use similar directory structures

2. Service Dependencies
   - Both require network configuration
   - Both need SELinux context management
   - Both use systemd for service management

Recommendations:

1. Create a base container role that handles:
   - Directory setup
   - SELinux configuration
   - Basic systemd integration
   - Network configuration

2. Extract common handlers to a shared handler file

3. Standardize variable naming conventions across roles:
   - Use consistent prefixes
   - Standardize boolean naming (enable_*vs*_enabled)

### Architecture Improvements

1. Create Base Roles

```yaml
# Example base role structure
roles/
  ├── base-container/
  │   ├── tasks/
  │   │   ├── directories.yml
  │   │   ├── selinux.yml
  │   │   └── systemd.yml
  │   └── defaults/
  └── base-network/
      └── tasks/
          └── setup.yml
```

2. Implement Role Dependencies

```yaml
# Example role dependency
dependencies:
  - role: base-container
    vars:
      container_name: "{{ service_name }}"
```

3. Create Shared Variable Files

```yaml
# group_vars/container_services.yml
common_dns_servers:
  - "1.1.1.1"
  - "8.8.8.8"
common_network: "ct-net"
```

### Security Considerations

1. TLS Configuration
   - Standardize TLS implementation across services
   - Create common TLS validation tasks

2. Authentication
   - Implement consistent password/secret management
   - Standardize environment variable handling

### Testing Recommendations

1. Add Verification Tasks
   - Implement health checks
   - Add service readiness probes
   - Create common testing tasks

2. Implement Molecule Tests
   - Add basic molecule test structure
   - Create shared test scenarios

## Next Steps

1. Immediate Actions
   - Address TODOs and SMELLs
   - Implement early exit mechanisms
   - Create base roles for common functionality

2. Medium-term Improvements
   - Extract common patterns to shared roles
   - Standardize variable naming
   - Implement testing framework

3. Long-term Goals
   - Create comprehensive documentation
   - Implement full test coverage
   - Create reusable role templates

==== CONTENT_END ====
==== FILE_START ====
PATH: docs/Env-vars-used.md
TITLE: Required Environment Variables
FILENAME: Env-vars-used.md
==== CONTENT_START ====
# Required Environment Variables

## Core System Variables

These variables are used across multiple roles:

```bash
# User Environment
XDG_RUNTIME_DIR          # Required for rootless container operations
ANSIBLE_USER_DIR         # User's home directory
ANSIBLE_USER_ID          # User ID for container operations
```

## Role-Specific Variables

### Elasticsearch

```bash
ELASTIC_PASSWORD         # Password for Elasticsearch superuser
                        # Default: 'changemeplease'
                        # Required: Yes
                        # Used in: elasticsearch/defaults/main.yml
```

### Mattermost

```bash
MM_DB_PASSWORD          # PostgreSQL database password
                        # Default: 'changemeplease'
                        # Required: Yes
                        # Used in: mattermost/defaults/main.yml
```

### Traefik

```bash
LINODE_TOKEN            # API token for Linode DNS integration
                        # Default: None
                        # Required: Yes
                        # Used in: traefik/tasks/prerequisites.yml
```

## Optional Variables

### Redis

```bash
REDIS_PASSWORD          # Password for Redis authentication
                        # Default: 'changeme'
                        # Required: No, but highly recommended
                        # Used in: redis/defaults/main.yml
```

## Example Environment File

Here's a template `.env` file that can be used to set these variables:

```bash
# Core System
export XDG_RUNTIME_DIR="/run/user/$(id -u)"

# Elasticsearch
export ELASTIC_PASSWORD="your_secure_elastic_password"

# Mattermost
export MM_DB_PASSWORD="your_secure_mattermost_db_password"

# Traefik
export LINODE_TOKEN="your_linode_api_token"

# Redis
export REDIS_PASSWORD="your_secure_redis_password"
```

## Usage Notes

1. Environment Variable Precendence:
   - Ansible playbook variables override environment variables
   - Environment variables override default values in role defaults

2. Security Considerations:
   - Don't store passwords in version control
   - Use Ansible Vault for sensitive values
   - Consider using a secrets management solution (e.g., HashiCorp Vault)

3. Validation:
   - All roles include validation tasks for required variables
   - Roles will fail early if required variables are not set

4. Integration:
   - Variables can be set in CI/CD pipelines
   - Can be managed through configuration management
   - Consider using `.env` files for local development

## Best Practices

1. Always use strong, unique passwords
2. Rotate credentials regularly
3. Use separate credentials for development and production
4. Document any changes to default values
5. Validate environment setup before running playbooks

==== CONTENT_END ====
==== FILE_START ====
PATH: docs/Solti-Container-Pattern.md
TITLE: Podman Container Service Pattern Guide
FILENAME: Solti-Container-Pattern.md
==== CONTENT_START ====
# Podman Container Service Pattern Guide

This guide documents the standard pattern for creating containerized services using Podman, based on proven implementations like Mattermost and Elasticsearch.

## Directory Structure

```
roles/
└── your_service/
    ├── defaults/
    │   └── main.yml         # Default variables
    ├── handlers/
    │   └── main.yml         # Service restart handlers
    ├── tasks/
    │   ├── main.yml         # Main task orchestration
    │   ├── prerequisites.yml # System preparations
    │   ├── containers.yml   # Container deployment
    │   ├── systemd.yml      # Systemd integration
    │   ├── tls.yml          # TLS configuration (optional)
    │   └── cleanup.yml      # Service removal tasks
    └── templates/
        ├── service.pod.j2           # Pod Quadlet
        ├── service-app.container.j2 # Main service Quadlet
        └── service-db.container.j2  # Database Quadlet (if needed)
```

## Standard Variables (defaults/main.yml)

```yaml
# Installation state
service_state: present
service_force_reload: false

# Container settings
service_image: "docker.io/org/image:tag"
service_data_dir: "{{ ansible_user_dir }}/service-data"
service_port: 8080

# Security settings
service_password: "changeme"
service_enable_security: true

# TLS settings
service_enable_tls: false
service_tls_cert_file: ""
service_tls_key_file: ""

# Cleanup settings
service_delete_data: false
```

## Implementation Steps

### 1. Prerequisites (prerequisites.yml)

```yaml
- name: Verify prerequisites
  assert:
    that:
      - service_password != "changeme"
      - service_data_dir is defined
    fail_msg: "Required variables not properly configured"

- name: Create service directories
  file:
    path: "{{ service_data_dir }}/{{ item }}"
    state: directory
    mode: "0750"
  loop:
    - ""  # Base directory
    - config
    - data
    - logs

- name: Configure SELinux (RHEL only)
  when: ansible_os_family == "RedHat"
  block:
    - name: Set SELinux context
      sefcontext:
        target: "{{ service_data_dir }}(/.*)?"
        setype: container_file_t
        state: present

    - name: Apply SELinux context
      command: restorecon -Rv "{{ service_data_dir }}"
```

### 2. Container Deployment (containers.yml)

```yaml
- name: Create service pod
  containers.podman.podman_pod:
    name: service-pod
    ports:
      - "127.0.0.1:{{ service_port }}:8080"

- name: Deploy service container
  containers.podman.podman_container:
    name: service-app
    pod: service-pod
    image: "{{ service_image }}"
    state: started
    volume:
      - "{{ service_data_dir }}/config:/config:Z,U"
      - "{{ service_data_dir }}/data:/data:Z,U"
    env:
      SERVICE_PASSWORD: "{{ service_password }}"
    restart_policy: always

- name: Verify container status
  command: podman ps --format {% raw %}"{{.Names}}"{% endraw %} --filter "pod=service-pod"
  register: container_status
  changed_when: false
```

### 3. Systemd Integration (systemd.yml)

```yaml
- name: Ensure systemd directories exist
  file:
    path: "{{ ansible_env.HOME }}/{{ item }}"
    state: directory
    mode: "0750"
  loop:
    - .config/systemd/user
    - .config/containers/systemd

- name: Template Quadlet files
  template:
    src: "{{ item.src }}"
    dest: "{{ ansible_env.HOME }}/.config/containers/systemd/{{ item.dest }}"
    mode: "0644"
  loop:
    - { src: service.pod.j2, dest: service.pod }
    - { src: service-app.container.j2, dest: service-app.container }

- name: Generate systemd units
  command:
    cmd: podman generate systemd --name service-pod --files --new
    chdir: "{{ ansible_env.HOME }}/.config/systemd/user"

- name: Enable and start services
  systemd:
    name: pod-service
    state: started
    enabled: yes
    scope: user
```

### 4. Cleanup Tasks (cleanup.yml)

```yaml
- name: Stop and disable services
  systemd:
    name: "pod-service"
    state: stopped
    enabled: no
    scope: user

- name: Remove pod and containers
  containers.podman.podman_pod:
    name: service-pod
    state: absent

- name: Remove systemd files
  file:
    path: "{{ item }}"
    state: absent
  loop:
    - "{{ ansible_env.HOME }}/.config/systemd/user/pod-service.service"
    - "{{ ansible_env.HOME }}/.config/containers/systemd/service.pod"
    - "{{ ansible_env.HOME }}/.config/containers/systemd/service-app.container"

- name: Remove data directory
  file:
    path: "{{ service_data_dir }}"
    state: absent
  when: service_delete_data | bool
```

## Quadlet Templates

### Pod Template (service.pod.j2)

```ini
[Pod]
Name=service-pod
PublishPort=127.0.0.1:${SERVICE_PORT}:8080

[Service]
Restart=always

[Install]
WantedBy=default.target
```

### Container Template (service-app.container.j2)

```ini
[Unit]
Description=Service Container
After=network-online.target

[Container]
Image={{ service_image }}
ContainerName=service-app
Pod=service-pod
Volume={{ service_data_dir }}/config:/config:Z,U
Volume={{ service_data_dir }}/data:/data:Z,U
Environment=SERVICE_PASSWORD={{ service_password }}

[Service]
Restart=always
TimeoutStartSec=300
TimeoutStopSec=70

[Install]
WantedBy=default.target
```

## Best Practices

1. **Security**:
   - Always bind ports to localhost (127.0.0.1)
   - Use secure passwords and TLS when possible
   - Follow principle of least privilege

2. **Data Management**:
   - Keep data in user's home directory
   - Use appropriate SELinux contexts
   - Implement backup strategies

3. **Systemd Integration**:
   - Use Quadlets for service definitions
   - Enable user lingering
   - Set appropriate timeouts

4. **Error Handling**:
   - Verify prerequisites
   - Check container status
   - Handle cleanup gracefully

5. **Platform Compatibility**:
   - Support both RHEL and Debian-based systems
   - Handle SELinux appropriately
   - Use distribution-specific package managers

## Verification

Include a verification playbook that checks:

- Pod and container status
- Service accessibility
- Basic functionality
- Configuration parameters

Example from Redis verification:

```yaml
- name: Verify service is running
  command: podman pod ps --format {% raw %}"{{.Name}}"{% endraw %}
  register: pod_status
  failed_when: "'service-pod' not in pod_status.stdout"
  changed_when: false

- name: Test service functionality
  uri:
    url: "http://localhost:{{ service_port }}/health"
    return_content: yes
  register: health_check
```

==== CONTENT_END ====
==== FILE_START ====
PATH: docs/ansible-tips.md
TITLE: tasks/common/create_directories.yml
FILENAME: ansible-tips.md
==== CONTENT_START ====
You've hit on a common pain point in Ansible. While it doesn't have traditional programming inheritance or function parameters, there are several patterns we can use to achieve similar goals. Let me outline some approaches:

1. **Reusable Task Files with `vars` include:**

```yaml
# tasks/common/create_directories.yml
- name: Create standard directories
  file:
    path: "{{ dir_base }}/{{ item }}"
    state: directory
    mode: "{{ dir_mode | default('0750') }}"
  loop: "{{ dir_list }}"

# Usage in role:
- name: Include directory creation
  include_tasks: common/create_directories.yml
  vars:
    dir_base: "{{ redis_data_dir }}"
    dir_list:
      - "config"
      - "data"
      - "logs"
```

2. **Role Defaults as "Parameters":**

```yaml
# roles/base_container/defaults/main.yml
container_base_dirs:
  - "config"
  - "data"
container_base_mode: "0750"

# roles/redis/meta/main.yml
dependencies:
  - role: base_container
    vars:
      container_base_dirs: "{{ redis_dirs }}"
```

3. **Using Facts as "Return Values":**

```yaml
# tasks/common/get_container_user.yml
- name: Get container user info
  stat:
    path: "{{ data_dir }}/config"
  register: dir_info

- name: Set container user facts
  set_fact:
    container_user_id: "{{ dir_info.stat.uid }}"
    container_group_id: "{{ dir_info.stat.gid }}"
```

4. **Import with Tags for "Inheritance":**

```yaml
# roles/base_service/tasks/main.yml
- import_tasks: prepare.yml
  tags: ["prepare"]
- import_tasks: configure.yml
  tags: ["configure"]

# roles/redis/tasks/main.yml
- import_tasks: roles/base_service/tasks/main.yml
- name: Redis-specific tasks
  tags: ["redis"]
  block:
    - import_tasks: redis_tasks.yml
```

5. **Using `defaults` Cascading for Optional Parameters:**

```yaml
# common_defaults.yml
service_port: "{{ specific_service_port | default(8080) }}"
service_user: "{{ specific_service_user | default(ansible_user) }}"
```

6. **Template Inheritance Through Includes:**

```yaml
# templates/base/service.conf.j2
[Service]
User={{ service_user }}
{% block service_config %}{% endblock %}

# templates/redis/service.conf.j2
{% include 'base/service.conf.j2' %}
{% block service_config %}
ExecStart=/usr/bin/redis-server
WorkingDirectory={{ redis_dir }}
{% endblock %}
```

7. **Using Group Variables as Common Configuration:**

```yaml
# group_vars/container_services/common.yml
common_dns_servers:
  - "1.1.1.1"
  - "8.8.8.8"
common_networks:
  - name: ct-net
    subnet: "172.24.0.0/16"

# roles/redis/defaults/main.yml
redis_dns_servers: "{{ common_dns_servers }}"
```

8. **Creating a Common Role Structure:**

```yaml
roles/
  ├── _base/  # Base role with common tasks
  │   ├── tasks/
  │   │   ├── directories.yml
  │   │   ├── selinux.yml
  │   │   └── systemd.yml
  │   └── defaults/
  │       └── main.yml
  └── redis/
      ├── meta/
      │   └── main.yml  # Depends on _base
      └── tasks/
          └── main.yml  # Redis-specific tasks
```

Here's how you might implement this for your container services:

```yaml
# roles/_base/tasks/prepare_container.yml
- name: Common container preparation
  block:
    - name: Get directory ownership
      stat:
        path: "{{ service_data_dir }}/config"
      register: dir_info
      
    - name: Create service directories
      file:
        path: "{{ service_data_dir }}/{{ item }}"
        state: directory
        mode: "{{ service_dir_mode | default('0750') }}"
        owner: "{{ dir_info.stat.uid | default(ansible_user_id) }}"
        group: "{{ dir_info.stat.gid | default(ansible_user_id) }}"
      loop: "{{ service_dirs }}"

# roles/redis/tasks/main.yml
- name: Prepare Redis
  include_tasks: roles/_base/tasks/prepare_container.yml
  vars:
    service_data_dir: "{{ redis_data_dir }}"
    service_dirs:
      - "config"
      - "data"
      - "logs"
```

While not perfect, these patterns can help make your Ansible code more maintainable and reusable. The key is to:

1. Use well-structured variable names
2. Leverage facts for passing data
3. Create reusable task files
4. Use role dependencies wisely
5. Take advantage of variable precedence

Would you like me to create an artifact showing how to refactor one of your specific roles using these patterns?

==== CONTENT_END ====
==== FILE_START ====
PATH: docs/es_resource_limits.md
TITLE: es_resource_limits
FILENAME: es_resource_limits.md
==== CONTENT_START ====
I see what you're asking about now. You've spotted two different ways of setting resource limits for Elasticsearch:

1. **Container-level limits** (using `ulimit` in Podman):

```yaml
ulimit:
  - "memlock=-1:-1"
  - "nofile=65535:65535"
```

2. **System-level limits** (using systemd configuration):

```yaml
content: |
  [Service]
  LimitMEMLOCK=infinity
  LimitNOFILE=65535
```

These are complementary approaches targeting different layers:

### Container-level ulimits

- Applied directly to the container process
- Set per-container
- Managed by Podman
- These limits can't exceed system-level limits

### Systemd-level limits

- Applied to the systemd user service
- Affects all processes started by the service
- Required because without these, the system limits would restrict what containers can do
- Works at a higher level than container limits

**Why both are needed:**

1. The systemd override ensures the user service has sufficient limits
2. The container limits ensure the specific container gets those limits

Elasticsearch is resource-intensive and needs special permissions:

- `memlock` prevents memory swapping (crucial for ES performance)
- `nofile` allows many open file handles (ES opens many files)

Using both approaches ensures the limits are properly propagated through all layers (system → systemd → podman → container). Without the systemd configuration, Podman might not be able to apply the container-level limits.

This is a best practice for running Elasticsearch in containers, especially with rootless Podman where permission boundaries are stricter.

==== CONTENT_END ====
==== FILE_START ====
PATH: docs/wazuh-agent-prompt.md
TITLE: Wazuh Agent Role Design Document
FILENAME: wazuh-agent-prompt.md
==== CONTENT_START ====
# Wazuh Agent Role Design Document

This document outlines the architecture, requirements, and implementation details for creating a complementary Wazuh agent role that works with the existing Wazuh server role.

## Role Structure

```
roles/
└── wazuh-agent/
    ├── defaults/
    │   └── main.yml          # Default variables
    ├── tasks/
    │   ├── main.yml          # Main tasks entry point
    │   ├── install.yml       # OS-specific installation tasks
    │   ├── configure.yml     # Configure agent
    │   ├── certificates.yml  # Certificate management
    │   └── register.yml      # Register with Wazuh manager
    ├── templates/
    │   └── ossec.conf.j2     # Agent configuration template
    ├── handlers/
    │   └── main.yml          # Service restart handlers
    └── vars/
        └── main.yml          # Internal variables
```

## Key Variables

```yaml
# Agent state
wazuh_agent_state: present     # Use 'absent' to remove

# Wazuh manager connection
wazuh_manager_host: "wazuh-server.example.com"
wazuh_manager_port: 1514
wazuh_registration_port: 1515

# Authentication method: 'password' or 'certificate'
wazuh_agent_auth_method: "certificate"

# Password-based registration
wazuh_enrollment_password: ""  # Should be passed from server

# Certificate-based authentication
wazuh_agent_cert_dir: "/var/ossec/etc/ssl"
wazuh_ca_cert_content: ""      # CA cert content from server
wazuh_agent_cert_content: ""   # Agent cert from server
wazuh_agent_key_content: ""    # Agent key from server

# Agent identification
wazuh_agent_name: "{{ inventory_hostname }}"
wazuh_agent_group: "default"

# Agent configuration
wazuh_agent_log_level: "info"
wazuh_agent_modules:
  - syscollector
  - sca
  - rootcheck
  - syscheck

# OS-specific packages
wazuh_repo_url:
  RedHat: "https://packages.wazuh.com/4.x/yum/"
  Debian: "https://packages.wazuh.com/4.x/apt/"

# Optional proxy settings
wazuh_agent_proxy_server: ""
wazuh_agent_proxy_port: 3128
wazuh_agent_proxy_user: ""
wazuh_agent_proxy_password: ""
```

## API Registration Workflow

For registering agents via the Wazuh API:

1. **Authentication**:
   - Obtain an API token from the Wazuh manager
   - Store token securely for subsequent operations

2. **Agent Registration**:
   - Register the agent with the manager via API
   - Retrieve the agent ID from the API response
   - Store agent ID for future operations

3. **Certificate Management** (if using certificate auth):
   - Generate agent certificate on the manager
   - Transfer certificates to the agent
   - Configure certificate paths in agent config

## CLI Registration Workflow

For registering agents via CLI commands:

1. **Agent ID Setup**:
   - Execute `manage_agents -a` to add agent on manager
   - Extract agent ID from command output

2. **Key Extraction**:
   - Get agent key using `manage_agents -e <agent_id>`
   - Transfer key to agent host

3. **Agent Import**:
   - Import key on agent using `manage_agents -i <key>`

## Certificate-Based Authentication Details

For certificate-based authentication:

1. **Certificate Requirements**:
   - CA certificate from Wazuh manager
   - Agent certificate with agent's ID in CN field
   - Agent private key

2. **File Locations**:
   - CA certificate: `/var/ossec/etc/rootca.pem`
   - Agent certificate: `/var/ossec/etc/client.cert`
   - Agent key: `/var/ossec/etc/client.key`

3. **Configuration Settings**:

   ```xml
   <client>
     <server>
       <address>{{ wazuh_manager_host }}</address>
       <port>{{ wazuh_manager_port }}</port>
       <protocol>tcp</protocol>
     </server>
     <crypto_method>aes</crypto_method>
     <key>/var/ossec/etc/client.key</key>
     <cert>/var/ossec/etc/client.cert</cert>
     <ca_store>/var/ossec/etc/rootca.pem</ca_store>
   </client>
   ```

## Integration Points with Server Role

The agent role will need certain information from the server role:

1. **Manager Information**:
   - Hostname/IP address
   - API port and credentials
   - Registration port

2. **Authentication Details**:
   - Enrollment password (for password-based auth)
   - CA certificate (for certificate-based auth)

3. **Agent Management**:
   - Group assignments
   - Policy configurations

## OS Support

The role should handle installation on various operating systems:

- RHEL/CentOS 7/8/9
- Debian 9/10/11/12
- Ubuntu 18.04/20.04/22.04
- Other platforms as needed

## Implementation Considerations

1. **Idempotency**:
   - Role should be rerunnable without errors
   - Should handle already registered agents

2. **Proxy Support**:
   - Include proxy configuration options
   - Support authenticated proxies

3. **Verification**:
   - Validate successful registration
   - Check agent-manager connection
   - Verify certificate validity

4. **Error Handling**:
   - Graceful failure with helpful messages
   - Retry logic for transient issues

5. **Security**:
   - Secure handling of credentials
   - Proper file permissions
   - Certificate validation

## Example Playbook Usage

```yaml
---
- name: Deploy Wazuh Agents
  hosts: agent_hosts
  vars:
    wazuh_manager_host: "wazuh.example.com"
    wazuh_agent_auth_method: "certificate"
    wazuh_ca_cert_content: "{{ lookup('file', '/path/to/rootca.pem') }}"
  roles:
    - role: wazuh-agent
```

This document provides a foundation for implementing a comprehensive Wazuh agent role that integrates with the existing server role, focusing on certificate-based authentication for enhanced security.

==== CONTENT_END ====
==== FILE_START ====
PATH: docs/wazuh-server-prompt.md
TITLE: Wazuh Podman Deployment with Ansible
FILENAME: wazuh-server-prompt.md
==== CONTENT_START ====
# Wazuh Podman Deployment with Ansible

This document outlines the structure and components needed for an Ansible role to deploy Wazuh using Podman containers.

## Architecture Overview

The deployment will consist of three main containers:

- **wazuh-manager**: Core Wazuh server with manager and API
- **wazuh-indexer**: Elasticsearch-based indexer for Wazuh data
- **wazuh-dashboard**: Web UI dashboard for visualization

## Directory Structure

```
roles/
└── wazuh-podman/
    ├── defaults/
    │   └── main.yml          # Default variables
    ├── tasks/
    │   ├── main.yml          # Main tasks entry point
    │   ├── prereq.yml        # Prerequisites (podman, directories, etc)
    │   ├── certificates.yml  # Certificate generation for components
    │   ├── manager.yml       # Wazuh manager container deployment
    │   ├── indexer.yml       # Wazuh indexer container deployment
    │   ├── dashboard.yml     # Wazuh dashboard container deployment
    │   └── post_setup.yml    # Post-installation configuration
    ├── templates/
    │   ├── manager/
    │   │   ├── ossec.conf.j2 # Wazuh manager main config
    │   │   └── local_rules.xml.j2
    │   ├── indexer/
    │   │   └── elasticsearch.yml.j2
    │   ├── dashboard/
    │   │   └── opensearch_dashboards.yml.j2
    │   └── podman/
    │       ├── wazuh-manager-container.yaml.j2 # Podman container definitions
    │       ├── wazuh-indexer-container.yaml.j2
    │       └── wazuh-dashboard-container.yaml.j2
    ├── handlers/
    │   └── main.yml          # Handlers for service restarts
    └── vars/
        └── main.yml          # Internal variables
```

## Key Variables

```yaml
# Container image versions
wazuh_manager_image: "wazuh/wazuh-manager:4.7.2"
wazuh_indexer_image: "wazuh/wazuh-indexer:4.7.2"
wazuh_dashboard_image: "wazuh/wazuh-dashboard:4.7.2"

# Network configuration
wazuh_network_name: "wazuh-network"
wazuh_network_subnet: "172.18.1.0/24"

# Ports
wazuh_api_port: 55000
wazuh_manager_port: 1514
wazuh_registration_port: 1515
wazuh_dashboard_port: 8082

# Volume locations
wazuh_data_dir: "/opt/wazuh/data"
wazuh_config_dir: "/opt/wazuh/config"
wazuh_backup_dir: "/opt/wazuh/backup"

# Resource limits
wazuh_indexer_memory: "2g"
wazuh_manager_memory: "1g"
wazuh_dashboard_memory: "1g"

# Certificates
wazuh_certs_dir: "/opt/wazuh/certs"
wazuh_generate_certs: true
wazuh_ca_cert_days: 3650
wazuh_cert_days: 3650

# Security settings
wazuh_admin_password: "{{ vault_wazuh_admin_password }}"
wazuh_api_user: "wazuh-api"
wazuh_api_password: "{{ vault_wazuh_api_password }}"
```

## Implementation Steps

### 1. Prerequisites

- Ensure Podman is installed and properly configured
- Create necessary directories for data persistence
- Set up container network for Wazuh services

### 2. Certificate Generation

- Generate certificates for secure communication between:
  - Wazuh manager and agents
  - Wazuh manager and indexer
  - Wazuh indexer nodes (if clustered)
  - Wazuh dashboard and indexer

### 3. Container Deployment

For each container:

- Create configuration files from templates
- Apply resource limits
- Set up persistent volumes
- Configure network settings
- Deploy containers using Podman

### 4. Post-Setup Configuration

- Initialize the Wazuh indexer
- Set up index patterns in the dashboard
- Create initial users
- Set up agent enrollment password
- Configure basic security policies

## Container Definitions

### Example: Wazuh Manager Container

```yaml
podman_create_args:
  name: "wazuh-manager"
  image: "{{ wazuh_manager_image }}"
  state: started
  restart_policy: "always"
  memory: "{{ wazuh_manager_memory }}"
  network:
    - name: "{{ wazuh_network_name }}"
  publish:
    - "{{ wazuh_manager_port }}:1514/tcp"
    - "{{ wazuh_registration_port }}:1515/tcp"
    - "{{ wazuh_api_port }}:55000/tcp"
  volume:
    - "{{ wazuh_config_dir }}/manager/ossec.conf:/var/ossec/etc/ossec.conf:Z"
    - "{{ wazuh_config_dir }}/manager/local_rules:/var/ossec/etc/rules/local_rules.xml:Z"
    - "{{ wazuh_data_dir }}/manager:/var/ossec/data:Z"
    - "{{ wazuh_certs_dir }}:/var/ossec/certs:Z"
  env:
    INDEXER_URL: "https://wazuh-indexer:9200"
    INDEXER_USERNAME: "admin"
    INDEXER_PASSWORD: "{{ wazuh_admin_password }}"
    FILEBEAT_SSL_VERIFICATION: "none"
```

## Security Considerations

- Use Ansible Vault for sensitive information
- Apply SELinux labels to mounted volumes
- Follow principle of least privilege for container configuration
- Enable TLS for all communications
- Store certificates securely

## Backup Strategy

- Regular backup of persistent data directories
- Certificate backup
- Configuration backup
- Consider point-in-time snapshot if available

## Monitoring Integration

- Configure container health checks
- Set up logging to external systems
- Monitor container resource usage

## Scaling Considerations

- Separate roles for manager, indexer, and dashboard for larger deployments
- Document cluster setup process for future scaling

==== CONTENT_END ====
==== FILE_START ====
PATH: plugins/README.md
TITLE: Collections Plugins Directory
FILENAME: README.md
==== CONTENT_START ====
# Collections Plugins Directory

This directory can be used to ship various plugins inside an Ansible collection. Each plugin is placed in a folder that
is named after the type of plugin it is in. It can also include the `module_utils` and `modules` directory that
would contain module utils and modules respectively.

Here is an example directory of the majority of plugins currently supported by Ansible:

```
└── plugins
    ├── action
    ├── become
    ├── cache
    ├── callback
    ├── cliconf
    ├── connection
    ├── filter
    ├── httpapi
    ├── inventory
    ├── lookup
    ├── module_utils
    ├── modules
    ├── netconf
    ├── shell
    ├── strategy
    ├── terminal
    ├── test
    └── vars
```

A full list of plugin types can be found at [Working With Plugins](https://docs.ansible.com/ansible-core/2.16/plugins/plugins.html).

==== CONTENT_END ====
==== FILE_START ====
PATH: roles/elasticsearch/Elasticvue-security.md
TITLE: Securing Elasticvue Access
FILENAME: Elasticvue-security.md
==== CONTENT_START ====

# Securing Elasticvue Access

## Security Considerations

Elasticvue provides a convenient dashboard for Elasticsearch but has no built-in authentication. This means:

- Anyone who can access the Elasticvue port can view the interface
- Browser-stored credentials could be used by other users
- No audit trail of who accessed the dashboard

## Recommended Security Solutions

### 1. Reverse Proxy Authentication

Set up Apache/nginx as a reverse proxy with authentication:

```apache
# Apache example
<Location "/elasticvue/">
    AuthType Basic
    AuthName "Restricted Access"
    AuthUserFile /etc/apache2/.htpasswd
    Require valid-user
    ProxyPass http://localhost:8080/
    ProxyPassReverse http://localhost:8080/
</Location>
```

```nginx
# Nginx example
location /elasticvue/ {
    auth_basic "Restricted Access";
    auth_basic_user_file /etc/nginx/.htpasswd;
    proxy_pass http://localhost:8080/;
}
```

### 2. Network Access Control

#### Option A: Localhost Only

Modify the Podman pod configuration to bind strictly to localhost:

```ini
[Pod]
Name=elasticsearch
PublishPort=127.0.0.1:${ELASTICSEARCH_PORT}:9200
PublishPort=127.0.0.1:${ELASTICSEARCH_GUI_PORT}:8080
```

#### Option B: VPN/Wireguard Access

- Only expose Elasticvue within a VPN network
- Configure Wireguard for secure remote access
- Restrict access to specific VPN IPs

### 3. Alternative Solutions

#### SSH Tunneling

Access Elasticvue through an SSH tunnel:

```bash
ssh -L 8080:localhost:8080 user@server
```

#### Alternative GUI Tools

Consider tools with built-in authentication:

- Kibana (official Elastic stack UI)
- Elasticsearch-HQ
- Dejavu

## Best Practices

1. Never expose Elasticvue directly to the internet
2. Use HTTPS when accessing through a reverse proxy
3. Implement IP-based access restrictions where possible
4. Use API keys with limited permissions instead of the elastic superuser
5. Regularly audit and rotate API keys
6. Monitor access logs for unauthorized attempts

## Implementation Notes

- Choose a security solution based on your infrastructure and requirements
- Consider combining multiple approaches (defense in depth)
- Document access procedures for team members
- Include security configurations in your Infrastructure as Code

==== CONTENT_END ====
==== FILE_START ====
PATH: roles/elasticsearch/README.md
TITLE: Elasticsearch Podman Role
FILENAME: README.md
==== CONTENT_START ====
# Elasticsearch Podman Role

This role manages the installation and configuration of Elasticsearch using rootless Podman containers. It includes optional TLS support and Elasticvue as a lightweight GUI interface.

## Features

- Rootless Podman deployment
- Optional TLS/SSL encryption
- Elasticvue GUI interface
- Systemd integration using Quadlets
- Configurable resource limits
- X-Pack security features

## Requirements

- Podman 4.x or later
- Systemd
- User with sudo access
- SELinux if running on RHEL/CentOS (role handles contexts)

## Role Variables

### Installation Options

```yaml
elasticsearch_state: present  # Use 'absent' to remove
elasticsearch_force_reload: false
elasticsearch_delete_data: false  # Set to true to delete data during removal
```

### Container Settings

```yaml
elasticsearch_image: "docker.io/elasticsearch:8.12.1"
elasticsearch_elasticvue_image: "docker.io/cars10/elasticvue:latest"
elasticsearch_port: 9200
elasticsearch_gui_port: 8080
```

### Security Settings

```yaml
elasticsearch_enable_security: true
elasticsearch_password: "change_this_password"
```

### TLS Configuration

```yaml
elasticsearch_enable_tls: false
elasticsearch_tls_cert_file: ""  # Path to your certificate
elasticsearch_tls_key_file: ""   # Path to your private key
elasticsearch_tls_min_version: "TLSv1.2"
elasticsearch_tls_verify_client: "optional"
```

### Resource Settings

```yaml
elasticsearch_memory: "1g"  # JVM heap size
```

See defaults/main.yml for all available variables and their default values.

## Example Playbooks

### Basic Installation

```yaml
- hosts: servers
  roles:
    - role: elasticsearch
      vars:
        elasticsearch_password: "secure_password"
        elasticsearch_memory: "2g"
```

### With TLS Enabled

```yaml
- hosts: servers
  roles:
    - role: elasticsearch
      vars:
        elasticsearch_password: "secure_password"
        elasticsearch_enable_tls: true
        elasticsearch_tls_cert_file: "/path/to/cert.pem"
        elasticsearch_tls_key_file: "/path/to/key.pem"
```

### Removal with Data Cleanup

```yaml
- hosts: servers
  roles:
    - role: elasticsearch
      vars:
        elasticsearch_state: absent
        elasticsearch_delete_data: true
```

## Usage

After deployment:

- Elasticsearch will be available at http(s)://localhost:9200
- Elasticvue GUI will be available at <http://localhost:8080>

### Initial Setup

1. Get the cluster status:

```bash
curl -X GET "localhost:9200/_cluster/health?pretty" -u elastic:${elasticsearch_password}
```

2. Access Elasticvue GUI:
   - Open <http://localhost:8080> in your browser
   - Connect to http(s)://localhost:9200
   - Use credentials: elastic / ${elasticsearch_password}

## Security Considerations

1. Default configuration enables X-Pack security
2. TLS is optional but recommended for production
3. Change default passwords after installation
4. Configure firewall rules as needed
5. Services bind to localhost by default

## License

MIT

## Author Information

Created by jackaltx and claude.

==== CONTENT_END ====
==== FILE_START ====
PATH: roles/hashivault/README.md
TITLE: HashiCorp Vault Role Documentation
FILENAME: README.md
==== CONTENT_START ====
# HashiCorp Vault Role Documentation

## Introduction

HashiCorp Vault is a powerful secrets management tool that provides secure storage for sensitive data such as API keys, passwords, certificates, and more. This Ansible role deploys Vault as a containerized service using Podman with a focus on ease of setup, management, and cleanup.

Key features of this implementation:

- Rootless Podman deployment
- Systemd integration using Quadlets
- Secure initialization and key management
- Multiple secret engines support (KV, PKI, SSH, Transit)
- Built-in web UI for easy management

This role follows the standard SOLTI container pattern, providing consistent deployment, configuration, and management commands across all services.

## Deployment Scenario

By default, the role deploys Vault on the local machine with the following configuration:

- **API Port**: 8200 (configurable via `vault_api_port`)
- **Cluster Port**: 8201 (configurable via `vault_cluster_port`)
- **UI Enabled**: Yes (access via <http://localhost:8200/ui>)
- **Storage**: File backend (configurable to Raft for clustering)
- **TLS**: Optional (disabled by default)

## Installation

### Prepare

The preparation step creates the necessary directory structure and configuration files. This step runs once per host:

```bash
./manage-svc.sh hashivault prepare
```

This command:

- Creates configuration and data directories
- Sets proper permissions
- Configures SELinux contexts (on RHEL-based systems)

Directory structure created:

```
~/vault-data/
├── config/    # Vault configuration files
├── data/      # Vault storage
├── logs/      # Audit logs
└── tls/       # TLS certificates (if enabled)
```

### Deploy

The deployment step creates and starts the Vault container:

```bash
./manage-svc.sh hashivault deploy
```

This command is idempotent and can be run multiple times safely. It:

- Creates the Podman pod and container(s)
- Generates systemd Quadlet files for service management
- Starts the Vault service as a rootless container

Quadlet files created:

```bash
$ ls -1 ~/.config/containers/systemd/vault*
/home/jackaltx/.config/containers/systemd/vault.pod
/home/jackaltx/.config/containers/systemd/vault-svc.container
```

To check the service status:

```bash
$ systemctl --user status vault-pod
# or
$ systemctl --user status vault-svc
```

Sample status output:

```
● vault-svc.service - HashiCorp Vault Container
 Loaded: loaded (/home/jackaltx/.config/containers/systemd/vault-svc.container; generated)
 Drop-In: /usr/lib/systemd/user/service.d
 └─10-timeout-abort.conf
 Active: active (running) since Mon 2025-03-03 15:29:24 CST; 1h 32min ago
 Invocation: 93fc66d5087c43e3abc57921aab5d52f
 Main PID: 3337 (conmon)
 Tasks: 14 (limit: 74224)
 Memory: 378.4M (peak: 379.6M)
 CPU: 2.965s
 CGroup: /user.slice/user-1000.slice/user@1000.service/app.slice/vault-svc.service
 ├─libpod-payload-b3fcc94cd6a1c263626ac6283d30ab854d9437379ec1a618b233c7c929891819
 │ ├─3346 /usr/bin/dumb-init /bin/sh /usr/local/bin/docker-entrypoint.sh server
 │ └─3359 vault server -config=/vault/config -dev-root-token-id= -dev-listen-address=0.0.0.0:8200
 └─runtime
 └─3337 /usr/bin/conmon --api-version 1 -c b3fcc94cd6a1c263626ac6283d30ab854d9437379ec1a618b233c7c929891819 -u b3fcc94cd6a1c263626ac6283d30ab854d9437379ec1a618b233c7c929891819 -r /usr/bin/c>
```

## Configuration

### Initialize Vault

After deployment, Vault must be initialized to generate encryption keys and root tokens:

```bash
./svc-exec.sh hashivault initialize
```

This command:

- Initializes Vault with a configurable number of key shares (default: 5)
- Sets the key threshold for unsealing (default: 3)
- Securely stores the keys and root token in `~/.secrets/vault-secrets/vault-keys.json`
- Optionally creates a backup of the keys
- Automatically unseals Vault if configured to do so

### Configure Vault

Set up Vault with authentication methods and secret engines:

```bash
./svc-exec.sh hashivault configure
```

This command configures:

- Authentication methods (userpass, approle)
- Admin user with secure password
- Policies for access control
- KV v2 secrets engine for general secrets

### Set Up Secret Engines

Enable and configure additional secret engines:

```bash
./svc-exec.sh hashivault vault-secrets
```

This task:

- Enables Transit engine for encryption as a service
- Enables PKI engine for certificate management
- Enables SSH engine for SSH key signing/management
- Populates initial secrets from environment variables or inventory variables

#### Initial Secrets Configuration

You can configure initial secrets in your inventory, host_vars, or group_vars:

```yaml
# Example vault_initial_secrets configuration
vault_initial_secrets:
  - path: "kv/ansible/vault"
    data:
      provision_vault_password: "{{ lookup('env', 'PROVISION_VAULT_PASSWORD') | default('changeme_in_production') }}"

  - path: "kv/services/elasticsearch"
    data:
      elastic_password: "{{ lookup('env', 'ELASTIC_PASSWORD') | default('changeme_in_production') }}"
      es_ro_token: "{{ lookup('env', 'ES_RO_TOKEN') | default('changeme_in_production') }}"

  # Additional secrets as needed
```

## Maintenance

### Unsealing Vault

Vault requires unsealing after every restart:

```bash
./svc-exec.sh hashivault unseal
```

This command:

- Checks if Vault is already unsealed
- Reads the unseal keys from the keys file
- Applies the keys up to the threshold
- Verifies the unsealed status

### Backup

To back up your Vault data:

```bash
./svc-exec.sh hashivault backup
```

This creates a timestamped backup of:

- Vault data directory
- Configuration files
- Unseal keys (encrypted)

### Remove Vault

To remove the Vault deployment (keeping data by default):

```bash
./manage-svc.sh hashivault remove
```

To completely remove Vault including all data:

```bash
VAULT_DELETE_DATA=true ./manage-svc.sh hashivault remove
```

## Security Considerations

1. **Key Storage**: Unseal keys and root tokens are extremely sensitive. In production:
   - Store key shares with different security officers
   - Use hardware security modules (HSMs) if possible
   - Never store all unseal keys on the same system as Vault

2. **Production Settings**:
   - Enable TLS (`vault_enable_tls: true`)
   - Use audit logging (`vault_enable_audit: true`)
   - Configure proper firewall rules
   - Use Raft storage instead of file storage for high availability

3. **Root Token**: The root token should be used only for initial setup, then revoked:

   ```bash
   podman exec -it vault-svc vault token revoke $ROOT_TOKEN
   ```

## Troubleshooting

### Cannot Unseal Vault

If you see "Error unsealing: Error making API request", check:

- Is Vault running? (`systemctl --user status vault-pod`)
- Is the keys file present and readable?
- Does the keys file contain valid unseal keys?

### API Connection Issues

If you can't connect to the Vault API:

- Check if Vault is running and unsealed
- Verify the API port is accessible (`curl -v http://localhost:8200/v1/sys/health`)
- Ensure the network connection is not blocked by a firewall

### Permission Issues

For SELinux-related issues:

- Check SELinux contexts: `ls -Z ~/vault-data`
- Reapply contexts: `sudo restorecon -Rv ~/vault-data`

## Configuration Variables

Key variables for customizing the Vault deployment:

```yaml
# Installation state
hashivault_state: present               # Use 'absent' to remove
vault_force_reload: false          # Force reload configuration
vault_delete_data: false           # Delete data on removal

# Container settings
vault_image: docker.io/hashicorp/vault:1.15
vault_data_dir: "{{ ansible_user_dir }}/vault-data"
vault_api_port: 8200
vault_cluster_port: 8201

# Security settings
vault_enable_ui: true
vault_enable_audit: true

# TLS Configuration
vault_enable_tls: false
vault_tls_cert_file: ""            # Path to certificate
vault_tls_key_file: ""             # Path to private key
vault_tls_ca_file: ""              # Optional CA certificate

# Storage settings
vault_storage_type: "file"         # Options: file, raft, consul
```

For complete configuration options, see `defaults/main.yml`.

==== CONTENT_END ====
==== FILE_START ====
PATH: roles/mattermost/README.md
TITLE: Mattermost Deployment with Podman
FILENAME: README.md
==== CONTENT_START ====
# Mattermost Deployment with Podman

This collection of playbooks and roles manages the deployment of Mattermost using rootless Podman containers. It supports both RHEL/CentOS and Debian-based systems.

## Features

- Rootless Podman deployment
- PostgreSQL database container
- Optional TLS support
- Systemd integration using Quadlets
- Cross-platform support (RHEL/CentOS and Debian)
- SELinux support for RHEL systems

## Prerequisites

- Ansible 2.9 or newer
- Target system running either:
  - RHEL/CentOS 9 or newer
  - Debian 12 (Bookworm) or newer
  - Ubuntu 22.04 or newer
- User with sudo access

## Quick Start

1. Prepare the host system:

```bash
ansible-playbook -K prepare-podman-host.yml
```

2. Deploy Mattermost:

```bash
ansible-playbook deploy-mattermost.yml
```

3. Remove Mattermost (keeping data):

```bash
ansible-playbook remove-mattermost.yml
```

## Configuration

### Default Variables

See `roles/mattermost/defaults/main.yml` for all available variables. Key variables include:

```yaml
mattermost_postgres_password: "change_this_password"
mattermost_port: 8065
mattermost_site_url: ""
mattermost_enable_tls: false
```

### TLS Configuration

To enable TLS, provide these variables:

```yaml
mattermost_enable_tls: true
mattermost_tls_cert_file: "/path/to/cert.pem"
mattermost_tls_key_file: "/path/to/key.pem"
```

## Platform-Specific Notes

### RHEL/CentOS

- Automatically enables required repositories (CRB)
- Configures SELinux contexts for container volumes
- Uses native podman-compose package

### Debian/Ubuntu

- Installs required dependencies via apt
- Installs podman-compose via pip
- No SELinux configuration needed

## Directory Structure

```
mattermost_data_dir/
├── config/
├── data/
├── logs/
├── plugins/
├── client/plugins/
└── postgres/
```

## Systemd Integration

The deployment uses Podman Quadlets for systemd integration. Services are installed in the user's systemd instance:

- pod-mattermost.service
- container-mattermost-db.service
- container-mattermost-svc.service

## Troubleshooting

1. DNS Issues:
   - The playbook configures DNS servers (1.1.1.1 and 8.8.8.8)
   - Check containers.conf if DNS issues persist

2. SELinux (RHEL only):
   - Container contexts are automatically set
   - Run `restorecon -Rv` on data directory if permissions issues occur

3. Podman:
   - Use `podman logs container-name` to check container logs
   - Verify user lingering is enabled: `loginctl user-status $USER`

## Maintenance

### Backup

1. Database:

```bash
podman exec mattermost-db pg_dump -U mmuser mattermost > backup.sql
```

2. Config and Data:

```bash
tar czf mattermost-data.tar.gz ~/mattermost-data
```

### Updates

1. Update container images:

```yaml
mattermost_image: "mattermost/mattermost-team-edition:latest"
mattermost_postgres_image: "postgres:13-alpine"
```

2. Redeploy:

```bash
ansible-playbook deploy-mattermost.yml
```

## License

MIT

## Author Information

Created by Jackaltx. Extended by the community.

==== CONTENT_END ====
==== FILE_START ====
PATH: roles/redis/debug-help.md
TITLE: Clauge debug help
FILENAME: debug-help.md
==== CONTENT_START ====
# Clauge debug help

I'll create a list of useful Traefik debugging commands, focusing on the API endpoints that help diagnose routing and configuration issues.

## Traefik API Debugging Commands

### Router Information

```bash
# List all routers and their configurations
curl http://localhost:9000/api/http/routers | jq .

# Get specific router details
curl http://localhost:9000/api/http/routers/your-router-name@docker | jq .
```

### Service Information

```bash
# List all services
curl http://localhost:9000/api/http/services | jq .

# Get specific service details
curl http://localhost:9000/api/http/services/your-service-name@docker | jq .
```

### Middleware Information

```bash
# List all middlewares
curl http://localhost:9000/api/http/middlewares | jq .

# Get specific middleware details
curl http://localhost:9000/api/http/middlewares/your-middleware-name@docker | jq .
```

### Entry Points Status

```bash
# List all entrypoints
curl http://localhost:9000/api/entrypoints | jq .
```

### Provider Information

```bash
# Get Docker provider information
curl http://localhost:9000/api/providers/docker | jq .

# List all providers
curl http://localhost:9000/api/providers | jq .
```

### TLS Information

```bash
# Get TLS configurations
curl http://localhost:9000/api/http/routers?search=tls | jq .

# Check TLS certificates
curl http://localhost:9000/api/tls/certificates | jq .
```

### Health and Version

```bash
# Health check
curl http://localhost:9000/api/health | jq .

# Version information
curl http://localhost:9000/api/version | jq .

# Ping test
curl http://localhost:9000/api/ping
```

### Real-time Monitoring

```bash
# Get overview of all HTTP requests
curl http://localhost:9000/api/overview | jq .

# Get current configuration
curl http://localhost:9000/api/rawdata | jq .
```

### Debug Headers

```bash
# Test routing with specific headers
curl -H "Host: your-domain.com" http://localhost:8080 -v
```

Would you like me to expand on any of these categories or add more specific debugging scenarios?

==== CONTENT_END ====
==== FILE_START ====
PATH: roles/wazuh/README.md
TITLE: Wazuh on podman
FILENAME: README.md
==== CONTENT_START ====
# Wazuh on podman

Not working.  Containers startup, but do not stay up.  communications is broken

==== CONTENT_END ====